{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFaFvp_9uAiG"
   },
   "source": [
    "# Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2sZSd9A6iob"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_MqehvncibZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLHHoT5EuEVo"
   },
   "source": [
    "# Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZOlWyAkcmT4"
   },
   "outputs": [],
   "source": [
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"])\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"trl\", \"transformers\", \"peft\", \"datasets\", \"bitsandbytes\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BghdKVyLtzlW"
   },
   "source": [
    "# Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCAX65SRjnE9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVr5ZwIUty7w"
   },
   "source": [
    "# CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbjKKAKkez-f"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH=2048\n",
    "DTYPE=torch.float16\n",
    "LOAD_IN_4BIT=True\n",
    "OUTPUT_DIR=\"/content/drive/MyDrive/final_model\"\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/checkpoints\"\n",
    "EPOCHS=3\n",
    "BATCH_SIZE=1\n",
    "GRADIENT_ACCUMULATION_STEPS=16\n",
    "LEARNING_RATE=2e-4\n",
    "SAVE_STEPS=1000\n",
    "DATASET_SIZE=8000\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR,exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9rh7SV5vcqQ"
   },
   "source": [
    "# DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb5UziAdvYQZ"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess(json_path,num_samples=8000):\n",
    "  data=[]\n",
    "\n",
    "  if json_path.endswith(\"jsonl\"):\n",
    "    with open(json_path,\"r\") as f:\n",
    "      for line in f:\n",
    "        data.append(json.loads(line))\n",
    "  else:\n",
    "    with open(json_path,\"r\") as f:\n",
    "      content=f.read()\n",
    "      data =json.loads(content)\n",
    "\n",
    "  data=data[:num_samples]\n",
    "  np.random.shuffle(data)\n",
    "\n",
    "  return data\n",
    "\n",
    "def format_prompt(example):\n",
    "  prompt=example['prompt']\n",
    "  completion=example['Completion']\n",
    "  formatted=f\"<s>[INST]{prompt}[/INST]{completion}</s>\"\n",
    "  return {\"text\":formatted}\n",
    "\n",
    "\n",
    "dataset_paths=[\n",
    "    \"/content/drive/MyDrive/Data/Custome_twitter.json\"\n",
    "]\n",
    "\n",
    "dataset_path=None\n",
    "for path in dataset_paths:\n",
    "  if os.path.exists(path):\n",
    "    dataset_path=path\n",
    "    break\n",
    "\n",
    "\n",
    "\n",
    "if dataset_path is None or not os.path.exists(dataset_path):\n",
    "    print(\"No dataset found at the specified path.\")\n",
    "else:\n",
    "    raw_data = load_and_preprocess(dataset_path, DATASET_SIZE)\n",
    "    formatted_data = [format_prompt(item) for item in raw_data]\n",
    "    dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in formatted_data]})\n",
    "    print(f\"Loaded Dataset - {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k62MpT01can"
   },
   "source": [
    " # MODEL LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1KYh50iV0O82"
   },
   "outputs": [],
   "source": [
    "model,tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "model=FastLanguageModel.get_peft_model(\n",
    "    model=model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Mode is loaded for Fine Tunning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6LoF5jABkUE"
   },
   "source": [
    "# CHECKPOINT DETECTION & RESUME LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IU78i6EO4t42"
   },
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(checkpoints_dir):\n",
    "  if os.path.exists(checkpoints_dir):\n",
    "    return None\n",
    "\n",
    "  checkpoints=[d for d in os.listdir(checkpoints_dir) if d.startswith(\"checkpoint-\")]\n",
    "  if not checkpoints:\n",
    "    return None\n",
    "\n",
    "\n",
    "  checkpoint_numbers=[int(d.split(\"-\")[1]) for d in checkpoints]\n",
    "  latest_num=max(checkpoint_numbers)\n",
    "  latest_checkpoint=os.path.join(checkpoints_dir,f\"checkpoint-{latest_num}\")\n",
    "  return latest_checkpoint\n",
    "\n",
    "\n",
    "resume_from_checkpoint=find_latest_checkpoint(CHECKPOINT_DIR)\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "  print(f\"Found checkpoint: {resume_from_checkpoint}\")\n",
    "else:\n",
    "  print(\"No checkpoint found. Starting fresh training...\")\n",
    "  resume_from_checkpoint = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHkrhcmnT5Oj"
   },
   "source": [
    "# TRAINING ARGUMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-Dqpgb6DIsQ"
   },
   "outputs": [],
   "source": [
    "trainning_args=TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    logging_steps=10,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=3,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    seed=42,\n",
    "    report_to=[\"tensorboard\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiqY9MXbVeur"
   },
   "source": [
    "# SFT TRAINER SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geZw5rNkVc7O"
   },
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=trainning_args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7NyuOLyV9Tz"
   },
   "source": [
    "#  TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IstVnuBhV692"
   },
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2aqlSG1WIyH"
   },
   "source": [
    "# SAVE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20Gq8CgZWH70"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7vdKsnr0O_r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
