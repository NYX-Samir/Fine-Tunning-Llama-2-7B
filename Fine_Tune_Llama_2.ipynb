{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Imports"
      ],
      "metadata": {
        "id": "LFaFvp_9uAiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t2sZSd9A6iob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0801199-63fd-419c-e299-03ccfae84e45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "e_MqehvncibZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "JLHHoT5EuEVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"trl\", \"transformers\", \"peft\", \"datasets\", \"bitsandbytes\"])\n"
      ],
      "metadata": {
        "id": "uZOlWyAkcmT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cabd8495-5044-43e4-bb77-0d6fcb86675c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports Libraries"
      ],
      "metadata": {
        "id": "BghdKVyLtzlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datasets import Dataset\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DCAX65SRjnE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURATION"
      ],
      "metadata": {
        "id": "tVr5ZwIUty7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH=2048\n",
        "DTYPE=torch.float16\n",
        "LOAD_IN_4BIT=True\n",
        "OUTPUT_DIR=\"/content/drive/MyDrive/final_model\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/checkpoints\"\n",
        "EPOCHS=3\n",
        "BATCH_SIZE=1\n",
        "GRADIENT_ACCUMULATION_STEPS=16\n",
        "LEARNING_RATE=2e-4\n",
        "SAVE_STEPS=1000\n",
        "DATASET_SIZE=8000\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR,exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_DIR,exist_ok=True)"
      ],
      "metadata": {
        "id": "LbjKKAKkez-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET LOADING"
      ],
      "metadata": {
        "id": "M9rh7SV5vcqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(json_path,num_samples=8000):\n",
        "  data=[]\n",
        "\n",
        "  if json_path.endswith(\"jsonl\"):\n",
        "    with open(json_path,\"r\") as f:\n",
        "      for line in f:\n",
        "        data.append(json.loads(line))\n",
        "  else:\n",
        "    with open(json_path,\"r\") as f:\n",
        "      content=f.read()\n",
        "      data =json.loads(content)\n",
        "\n",
        "  data=data[:num_samples]\n",
        "  np.random.shuffle(data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def format_prompt(example):\n",
        "  prompt=example['prompt']\n",
        "  completion=example['Completion']\n",
        "  formatted=f\"<s>[INST]{prompt}[/INST]{completion}</s>\"\n",
        "  return {\"text\":formatted}\n",
        "\n",
        "\n",
        "dataset_paths=[\n",
        "    \"/content/drive/MyDrive/Data/Custome_twitter.json\"\n",
        "]\n",
        "\n",
        "dataset_path=None\n",
        "for path in dataset_paths:\n",
        "  if os.path.exists(path):\n",
        "    dataset_path=path\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "if dataset_path is None or not os.path.exists(dataset_path):\n",
        "    print(\"No dataset found at the specified path.\")\n",
        "else:\n",
        "    raw_data = load_and_preprocess(dataset_path, DATASET_SIZE)\n",
        "    formatted_data = [format_prompt(item) for item in raw_data]\n",
        "    dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in formatted_data]})\n",
        "    print(f\"Loaded Dataset - {len(dataset)} samples\")\n"
      ],
      "metadata": {
        "id": "sb5UziAdvYQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # MODEL LOADING"
      ],
      "metadata": {
        "id": "1k62MpT01can"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model,tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "model=FastLanguageModel.get_peft_model(\n",
        "    model=model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Mode is loaded for Fine Tunning\")"
      ],
      "metadata": {
        "id": "1KYh50iV0O82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHECKPOINT DETECTION & RESUME LOGIC"
      ],
      "metadata": {
        "id": "j6LoF5jABkUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_latest_checkpoint(checkpoints_dir):\n",
        "  if os.path.exists(checkpoints_dir):\n",
        "    return None\n",
        "\n",
        "  checkpoints=[d for d in os.listdir(checkpoints_dir) if d.startswith(\"checkpoint-\")]\n",
        "  if not checkpoints:\n",
        "    return None\n",
        "\n",
        "\n",
        "  checkpoint_numbers=[int(d.split(\"-\")[1]) for d in checkpoints]\n",
        "  latest_num=max(checkpoint_numbers)\n",
        "  latest_checkpoint=os.path.join(checkpoints_dir,f\"checkpoint-{latest_num}\")\n",
        "  return latest_checkpoint\n",
        "\n",
        "\n",
        "resume_from_checkpoint=find_latest_checkpoint(CHECKPOINT_DIR)\n",
        "\n",
        "if resume_from_checkpoint:\n",
        "  print(f\"Found checkpoint: {resume_from_checkpoint}\")\n",
        "else:\n",
        "  print(\"No checkpoint found. Starting fresh training...\")\n",
        "  resume_from_checkpoint = None"
      ],
      "metadata": {
        "id": "IU78i6EO4t42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING ARGUMENTS"
      ],
      "metadata": {
        "id": "DHkrhcmnT5Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainning_args=TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    logging_steps=10,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=3,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    max_grad_norm=1.0,\n",
        "    seed=42,\n",
        "    report_to=[\"tensorboard\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "z-Dqpgb6DIsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SFT TRAINER SETUP"
      ],
      "metadata": {
        "id": "aiqY9MXbVeur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    args=trainning_args,\n",
        "    packing=False,\n",
        ")"
      ],
      "metadata": {
        "id": "geZw5rNkVc7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  TRAINING"
      ],
      "metadata": {
        "id": "f7NyuOLyV9Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)"
      ],
      "metadata": {
        "id": "IstVnuBhV692"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE FINAL MODEL"
      ],
      "metadata": {
        "id": "a2aqlSG1WIyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "20Gq8CgZWH70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/final_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
        "\n",
        "text = \"Is it new Iphone worthy to buy\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "b7vdKsnr0O_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea914b89-d2ef-4af0-b081-1e370d628dbb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is it new Iphone worthy to buy?\n",
            "The newest Iphone is out. Is it worth to buy?\n",
            "Yes, it is. If you can afford it.\n",
            "Yeah I have the 6s and it’s good. I’m thinking of getting the 7 plus but I don’t know if I want to spend that much on a phone.\n",
            "@123311 it’s definitely worth it. I’ve had my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WqTKlGi7QdZy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}